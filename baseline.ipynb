{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner_base as nerbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # DONE: generating training set\n",
    "# import json\n",
    "\n",
    "# f = open('data/train','w')\n",
    "\n",
    "# with open('data/buzs_labeled.json') as json_file:\n",
    "#     for line in json_file:\n",
    "#         review = json.loads(line)\n",
    "#         for tuple in review[\"text\"]:\n",
    "#             tuple = tuple[0] + \"\\t\" + tuple[1] + \"\\n\"\n",
    "#             data = tuple.encode(\"ascii\",\"ignore\")\n",
    "#             if data.strip() == \"-\":\n",
    "#                 data = \"\\n\"\n",
    "#             f.write(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = 10000\n",
    "\n",
    "# # TODO: add food word\n",
    "\n",
    "# # generating vocab\n",
    "# fv = open('data/vocab.txt', 'w')\n",
    "# fv.write(\"UUUNKKK\\n\")\n",
    "# fv.write(\"<s>\\n\")\n",
    "# fv.write(\"</s>\\n\")\n",
    "\n",
    "# cnt = 0\n",
    "# with open('data/vocabulary.txt') as vocab_file:\n",
    "#     for line in vocab_file:\n",
    "#         if cnt < VOCAB_SIZE:\n",
    "#             word = line.split(' ')[0] + '\\n'\n",
    "#             if word.strip() != '':\n",
    "#                 fv.write(word.encode(\"ascii\",\"ignore\"))\n",
    "#         cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop word and UUUNKKK should include in the dictionary\n",
    "word_to_num, num_to_word = nerbase.load_wv('data/vocab.txt')\n",
    "\n",
    "tagnames = [\"O\", \"B\", \"I\", \"L\", \"U\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "wsize = 3\n",
    "pad = (wsize - 1)/2\n",
    "\n",
    "X_train, y_train, words_train = du.generateData('data/train_small', word_to_num, tag_to_num, wsize)\n",
    "# X_dev, y_dev, words_dev = du.generateData('data/dev_small', word_to_num, tag_to_num, wsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95230\n",
      "95230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####! not same length ###\n",
    "print len(X_train)\n",
    "print len(words_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# baseline features:\n",
    "\n",
    "# context\n",
    "# capitalize word\n",
    "# previous tokens...\n",
    "##### FEATURES TO ADD #####\n",
    "def isCapital(x):\n",
    "    if len(x) > 0:\n",
    "        return x[0].isupper()\n",
    "    return false\n",
    "\n",
    "def extractFeatures(X, words, windowsize, word_to_num):\n",
    "    fv1 = contextFeature(X, windowsize, word_to_num)\n",
    "    fv2 = capitalFeature(words)\n",
    "    fv = column_stack((fv1,fv2))\n",
    "    return fv2\n",
    "\n",
    "def contextFeature(X, windowsize, word_to_num):\n",
    "    m = len(word_to_num)\n",
    "    onehot_x = identity(m)\n",
    "\n",
    "    wdim = m * windowsize\n",
    "    N = X.shape[0]\n",
    "    fvec = onehot_x[X].reshape(N, wdim)\n",
    "    return fvec\n",
    "\n",
    "def capitalFeature(words):\n",
    "    fvec = array([isCapital(ww) for ww in words]).astype(int)\n",
    "    return fvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num)\n",
    "def trainSubset(size, X_train, y_train, words_train):\n",
    "    X_train_s = X_train[:size]\n",
    "    y_train_s = y_train[:size]\n",
    "    words_train_s = words_train[:size]\n",
    "    return X_train_s, y_train_s, words_train_s\n",
    "\n",
    "\n",
    "# TRAIN_SIZE = 500\n",
    "# X_s, y_s, w_s = trainSubset(TRAIN_SIZE, X_train, y_train, words_train)\n",
    "X_s = X_train\n",
    "y_s = y_train\n",
    "w_s = words_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv = extractFeatures(X_s, w_s, wsize, word_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### just trying###\n",
    "X = X_train\n",
    "m = len(word_to_num)\n",
    "onehot_x = identity(m)\n",
    "\n",
    "wdim = m * wsize\n",
    "N = X.shape[0]\n",
    "fvec = onehot_x[X].reshape(N, wdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95230,)\n"
     ]
    }
   ],
   "source": [
    "print fv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using sklearn logistic regression package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      1.00      0.99       430\n",
      "          B       1.00      0.79      0.88        19\n",
      "          I       1.00      0.85      0.92        13\n",
      "          L       1.00      0.84      0.91        19\n",
      "          U       1.00      0.79      0.88        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97       500\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(fv, y_s)\n",
    "y_pred = lr.predict(fv)\n",
    "print nerbase.full_report(y_s, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8486\n",
      "9082\n"
     ]
    }
   ],
   "source": [
    "print len(X_dev)\n",
    "print len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29989)\n",
      "(8486, 29989)\n"
     ]
    }
   ],
   "source": [
    "print fv.shape\n",
    "print fvdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing using dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.88      1.00      0.93      7392\n",
      "          B       0.25      0.00      0.00       403\n",
      "          I       0.00      0.00      0.00       196\n",
      "          L       0.00      0.00      0.00       403\n",
      "          U       0.09      0.05      0.07        92\n",
      "\n",
      "avg / total       0.78      0.87      0.81      8486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fvdev = extractFeatures(X_dev, words_dev, windowsize, word_to_num)\n",
    "yp = lr.predict(fvdev)\n",
    "\n",
    "nerbase.full_report(y_dev, yp, tagnames) # full report, helpful diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
