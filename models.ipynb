{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner_base as nerbase\n",
    "import data_utils.ner as ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # DONE: generating training set\n",
    "# import json\n",
    "\n",
    "# f = open('data/train','w')\n",
    "\n",
    "# with open('data/buzs_labeled.json') as json_file:\n",
    "#     for line in json_file:\n",
    "#         review = json.loads(line)\n",
    "#         for tuple in review[\"text\"]:\n",
    "#             tuple = tuple[0] + \"\\t\" + tuple[1] + \"\\n\"\n",
    "#             data = tuple.encode(\"ascii\",\"ignore\")\n",
    "#             if data.strip() == \"-\":\n",
    "#                 data = \"\\n\"\n",
    "#             f.write(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = 10000\n",
    "\n",
    "# # TODO: add food word\n",
    "\n",
    "# # generating vocab\n",
    "# fv = open('data/vocab.txt', 'w')\n",
    "# fv.write(\"UUUNKKK\\n\")\n",
    "# fv.write(\"<s>\\n\")\n",
    "# fv.write(\"</s>\\n\")\n",
    "\n",
    "# cnt = 0\n",
    "# with open('data/vocabulary.txt') as vocab_file:\n",
    "#     for line in vocab_file:\n",
    "#         if cnt < VOCAB_SIZE:\n",
    "#             word = line.split(' ')[0] + '\\n'\n",
    "#             if word.strip() != '':\n",
    "#                 fv.write(word.encode(\"ascii\",\"ignore\"))\n",
    "#         cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop word and UUUNKKK should include in the dictionary\n",
    "# Load the starter word vectors\n",
    "wv, word_to_num, num_to_word = ner.load_wv('data/vocab.txt',\n",
    "                                           'data/wordVectors.txt')\n",
    "\n",
    "tagnames = [\"O\", \"B\", \"I\", \"L\", \"U\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "wsize = 5\n",
    "pad = (wsize - 1)/2\n",
    "\n",
    "X_train, y_train, words_train = du.generateData('data/train_v2', word_to_num, tag_to_num, wsize)\n",
    "X_dev, y_dev, words_dev = du.generateData('data/dev_v2', word_to_num, tag_to_num, wsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' '<s>' 'Excellent' 'Chinese' 'food']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print words_train[0]\n",
    "word_to_num['<s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', '<s>', 'excellent', 'chinese', 'food'], ['<s>', 'excellent', 'chinese', 'food', ','], ['excellent', 'chinese', 'food', ',', 'although'], ['chinese', 'food', ',', 'although', 'there'], ['food', ',', 'although', 'there', 'was'], [',', 'although', 'there', 'was', 'too'], ['although', 'there', 'was', 'too', 'much'], ['there', 'was', 'too', 'much', 'broccoli'], ['was', 'too', 'much', 'broccoli', 'in'], ['too', 'much', 'broccoli', 'in', 'my'], ['much', 'broccoli', 'in', 'my', 'beef'], ['broccoli', 'in', 'my', 'beef', 'and'], ['in', 'my', 'beef', 'and', 'broccoli'], ['my', 'beef', 'and', 'broccoli', 'for'], ['beef', 'and', 'broccoli', 'for', 'my'], ['and', 'broccoli', 'for', 'my', 'taste'], ['broccoli', 'for', 'my', 'taste', '.'], ['for', 'my', 'taste', '.', '</s>'], ['my', 'taste', '.', '</s>', '</s>'], ['<s>', '<s>', 'the', 'food', 'is']]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'O', 'O', 'B', 'I', 'L', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print [[num_to_word[x] for x in X_train[i]] for i in range(20)]\n",
    "print [tagnames[y_train[i]] for i in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Merge \n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "####! not same length ###\n",
    "print len(X_train)\n",
    "def one_hot(dim, x):\n",
    "    res = zeros(dim)\n",
    "    res[x] = 1.0\n",
    "    return res\n",
    "Y_train = [one_hot(5, x) for x in y_train]\n",
    "Y_dev = [one_hot(5, x) for x in y_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordvector_neighbors(idxs, wordVecs, num_to_word, n=10):\n",
    "    res_list = []\n",
    "    for idx in idxs:\n",
    "        #print square(wordVecs - wordVecs[idx]).shape\n",
    "        res = argsort(sum(square(wordVecs - wordVecs[idx]), axis=1))[:n+1]\n",
    "        res_list.append([num_to_word[x] for x in res])\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "385851/385851 [==============================] - 79s - loss: 0.2946    \n",
      "385851/385851 [==============================] - 79s - loss: 0.2946    \n",
      "385851/385851 [==============================] - 79s - loss: 0.2946    \n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "385851/385851 [==============================] - 76s - loss: 0.2525    \n",
      "385851/385851 [==============================] - 76s - loss: 0.2525    \n",
      "385851/385851 [==============================] - 76s - loss: 0.2525    \n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "385851/385851 [==============================] - 75s - loss: 0.2383    \n",
      "385851/385851 [==============================] - 75s - loss: 0.2383    \n",
      "385851/385851 [==============================] - 75s - loss: 0.2383    \n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "385851/385851 [==============================] - 72s - loss: 0.2291    \n",
      "385851/385851 [==============================] - 72s - loss: 0.2291    \n",
      "385851/385851 [==============================] - 72s - loss: 0.2291    \n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "385851/385851 [==============================] - 74s - loss: 0.2227    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2227    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2227    \n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "385851/385851 [==============================] - 74s - loss: 0.2176    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2176    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2176    \n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "385851/385851 [==============================] - 74s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2137    \n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "385851/385851 [==============================] - 73s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2102    \n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "385851/385851 [==============================] - 74s - loss: 0.2074    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2074    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2074    \n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "385851/385851 [==============================] - 74s - loss: 0.2050    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2050    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2050    \n",
      "Epoch 10\n",
      "Epoch 10\n",
      "Epoch 10\n",
      "385851/385851 [==============================] - 73s - loss: 0.2028    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2028    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2028    \n",
      "Epoch 11\n",
      "Epoch 11\n",
      "Epoch 11\n",
      "385851/385851 [==============================] - 73s - loss: 0.2009    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2009    \n",
      "385851/385851 [==============================] - 73s - loss: 0.2009    \n",
      "Epoch 12\n",
      "Epoch 12\n",
      "Epoch 12\n",
      "385851/385851 [==============================] - 89s - loss: 0.1993    \n",
      "385851/385851 [==============================] - 89s - loss: 0.1993    \n",
      "385851/385851 [==============================] - 89s - loss: 0.1993    \n",
      "Epoch 13\n",
      "Epoch 13\n",
      "Epoch 13\n",
      "385851/385851 [==============================] - 73s - loss: 0.1978    \n",
      "385851/385851 [==============================] - 73s - loss: 0.1978    \n",
      "385851/385851 [==============================] - 73s - loss: 0.1978    \n",
      "Epoch 14\n",
      "Epoch 14\n",
      "Epoch 14\n",
      "385851/385851 [==============================] - 85s - loss: 0.1962    \n",
      "385851/385851 [==============================] - 85s - loss: 0.1962    \n",
      "385851/385851 [==============================] - 85s - loss: 0.1962    \n",
      "Epoch 15\n",
      "Epoch 15\n",
      "Epoch 15\n",
      "385851/385851 [==============================] - 83s - loss: 0.1949    \n",
      "385851/385851 [==============================] - 83s - loss: 0.1949    \n",
      "385851/385851 [==============================] - 83s - loss: 0.1949    \n",
      "Epoch 16\n",
      "Epoch 16\n",
      "Epoch 16\n",
      "385851/385851 [==============================] - 81s - loss: 0.1937    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1937    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1937    \n",
      "Epoch 17\n",
      "Epoch 17\n",
      "Epoch 17\n",
      "385851/385851 [==============================] - 81s - loss: 0.1925    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1925    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1925    \n",
      "Epoch 18\n",
      "Epoch 18\n",
      "Epoch 18\n",
      "385851/385851 [==============================] - 90s - loss: 0.1914    \n",
      "385851/385851 [==============================] - 90s - loss: 0.1914    \n",
      "385851/385851 [==============================] - 90s - loss: 0.1914    \n",
      "Epoch 19\n",
      "Epoch 19\n",
      "Epoch 19\n",
      "385851/385851 [==============================] - 81s - loss: 0.1906    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1906    \n",
      "385851/385851 [==============================] - 81s - loss: 0.1906    \n",
      "CPU times: user 24min 58s, sys: 1min 22s, total: 26min 21s\n",
      "Wall time: 26min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': [],\n",
       " 'epoch': 19,\n",
       " 'epoch_size': [],\n",
       " 'loss': [0.29455469539686163,\n",
       "  0.54701436746435261,\n",
       "  0.7853086649070159,\n",
       "  1.0144396609847954,\n",
       "  1.2371569258297939,\n",
       "  1.4547914335226764,\n",
       "  1.6684493525189594,\n",
       "  1.8786952083658601,\n",
       "  2.0860618742544093,\n",
       "  2.2910191378175875,\n",
       "  2.4938668459518758,\n",
       "  2.6947639561444072,\n",
       "  2.8940611242983172,\n",
       "  3.0918469220123685,\n",
       "  3.2880223534773347,\n",
       "  3.4829138309708059,\n",
       "  3.6765825060864454,\n",
       "  3.8691151073745749,\n",
       "  4.0605256469672382,\n",
       "  4.2510891045405526],\n",
       " 'val_accuracy': [],\n",
       " 'val_loss': []}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "windowSize = 5\n",
    "middleDim = 300\n",
    "ntrain = len(X_train)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding = Embedding(wv.shape[0], wv.shape[1], weights=[wv])\n",
    "hidden = Dense(wv.shape[1] * windowSize, middleDim, W_regularizer = l2(.001))\n",
    "output = Dense(middleDim, 5, W_regularizer = l2(.001))\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Flatten())\n",
    "model.add(hidden)\n",
    "model.add(Activation('tanh'))\n",
    "model.add(output)\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "%time model.fit(X_train[:ntrain], Y_train[:ntrain], nb_epoch=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named h5py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-12aefbd1d371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model1_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/Keras-0.0.1-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;31m# Save weights from all layers to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# if file exists and should not be overwritten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named h5py"
     ]
    }
   ],
   "source": [
    "model.save_weights('model1_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385851/385851 [==============================] - 7s     \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9556    0.9688    0.9621    331052\n",
      "          B     0.7410    0.6867    0.7128     15705\n",
      "          I     0.6776    0.5582    0.6122      5385\n",
      "          L     0.7270    0.6900    0.7080     15705\n",
      "          U     0.6960    0.6306    0.6617     18004\n",
      "\n",
      "avg / total     0.9215    0.9245    0.9227    385851\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  71.60%\n",
      "Mean recall:     65.66%\n",
      "Mean F1:         68.48%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_train)\n",
    "nerbase.full_report(y_train, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_train, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385851/385851 [==============================] - 9s     \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9509    0.9685    0.9596    331052\n",
      "          B     0.7303    0.6457    0.6854     15705\n",
      "          I     0.6438    0.5071    0.5674      5385\n",
      "          L     0.6852    0.7087    0.6968     15705\n",
      "          U     0.7123    0.5664    0.6311     18004\n",
      "\n",
      "avg / total     0.9157    0.9196    0.9170    385851\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  70.30%\n",
      "Mean recall:     62.41%\n",
      "Mean F1:         65.92%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_train)\n",
    "nerbase.full_report(y_train, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_train, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48184/48184 [==============================] - 0s     \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9298    0.9655    0.9473     41047\n",
      "          B     0.6756    0.5387    0.5994      2157\n",
      "          I     0.6359    0.4163    0.5032       663\n",
      "          L     0.6641    0.5535    0.6038      2157\n",
      "          U     0.6099    0.4546    0.5210      2160\n",
      "\n",
      "avg / total     0.8882    0.8975    0.8912     48184\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  64.86%\n",
      "Mean recall:     50.64%\n",
      "Mean F1:         56.81%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_dev)\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48184/48184 [==============================] - 0s     \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9278    0.9664    0.9467     41047\n",
      "          B     0.6716    0.5443    0.6013      2157\n",
      "          I     0.6054    0.3725    0.4613       663\n",
      "          L     0.6468    0.5841    0.6139      2157\n",
      "          U     0.6589    0.4051    0.5017      2160\n",
      "\n",
      "avg / total     0.8873    0.8970    0.8897     48184\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  65.41%\n",
      "Mean recall:     49.82%\n",
      "Mean F1:         56.20%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_dev)\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "[('<s>', 'O', 'O'), ('for', 'O', 'O'), ('the', 'O', 'O'), ('entree', 'O', 'O'), (',', 'O', 'O'), ('my', 'O', 'O'), ('wife', 'O', 'O'), ('ordered', 'U', 'O'), ('the', 'O', 'O'), ('``', 'U', 'O'), ('UUUNKKK', 'O', 'U'), (\"''\", 'O', 'O'), ('UUUNKKK', 'O', 'O'), ('$', 'O', 'O'), ('dd', 'O', 'O'), ('UUUNKKK', 'O', 'O'), (',', 'O', 'O'), ('which', 'O', 'O'), ('is', 'O', 'O'), ('curry', 'O', 'U'), ('lamb', 'O', 'O'), ('with', 'O', 'O'), ('fresh', 'O', 'O'), ('onions', 'U', 'O'), (',', 'O', 'O'), ('tomatoes', 'B', 'O'), (',', 'L', 'O'), ('garlic', 'O', 'O'), (',', 'O', 'O'), ('and', 'O', 'O'), ('ginger', 'O', 'O'), ('simmered', 'O', 'O'), ('thick', 'O', 'O'), ('sauce', 'B', 'O'), ('.', 'L', 'O'), ('</s>', 'O', 'O')]\n",
      "===============\n",
      "[('<s>', 'O', 'O'), ('after', 'O', 'O'), ('the', 'O', 'O'), ('uni', 'O', 'O'), (',', 'O', 'O'), ('squid', 'O', 'U'), (',', 'O', 'O'), ('and', 'O', 'O'), ('quail', 'O', 'B'), ('egg', 'O', 'L'), ('sushi', 'O', 'O'), (',', 'O', 'O'), ('we', 'O', 'O'), ('had', 'B', 'O'), ('our', 'L', 'O'), ('main', 'O', 'O'), ('sushi', 'O', 'O'), ('and', 'O', 'O'), ('sashimi', 'O', 'O'), ('platter', 'O', 'O'), ('.', 'O', 'O'), ('</s>', 'O', 'O')]\n",
      "===============\n",
      "[('<s>', 'O', 'O'), ('the', 'O', 'O'), ('small', 'O', 'O'), ('batch', 'O', 'O'), ('coffee', 'O', 'U'), ('was', 'O', 'O'), ('a', 'O', 'O'), ('little', 'O', 'O'), ('sweeter', 'O', 'O'), ('than', 'O', 'O'), ('your', 'O', 'O'), ('standard', 'O', 'O'), ('coffee', 'O', 'U'), ('shop', 'O', 'O'), ('.', 'O', 'O'), ('</s>', 'O', 'O')]\n",
      "===============\n",
      "[('<s>', 'O', 'O'), ('i', 'O', 'O'), ('will', 'O', 'O'), ('be', 'O', 'O'), ('thinking', 'B', 'O'), ('about', 'L', 'O'), ('that', 'O', 'O'), ('machaca', 'O', 'U'), ('green', 'O', 'U'), ('taco', 'O', 'O'), ('until', 'U', 'O'), ('we', 'O', 'O'), ('meet', 'O', 'O'), ('again', 'O', 'O'), ('.', 'O', 'O'), ('</s>', 'O', 'O')]\n",
      "===============\n",
      "[('<s>', 'O', 'O'), ('ordered', 'O', 'O'), ('the', 'O', 'O'), ('$', 'O', 'O'), ('UUUNKKK', 'O', 'O'), ('UUUNKKK', 'O', 'O'), ('combo', 'I', 'O'), (',', 'O', 'O'), ('which', 'O', 'O'), ('comes', 'B', 'O'), ('with', 'I', 'O'), ('d', 'U', 'B'), ('chicken', 'O', 'I'), ('fingers', 'O', 'L'), (',', 'O', 'O'), ('d', 'O', 'O'), ('texas', 'O', 'B'), ('toast', 'O', 'L'), (',', 'O', 'O'), ('fries', 'O', 'O'), (',', 'O', 'O'), ('cole', 'O', 'O'), ('slaw', 'O', 'O'), (',', 'O', 'O'), ('and', 'O', 'O'), ('a', 'O', 'O'), ('large', 'O', 'B'), ('drink', 'O', 'L'), (',', 'O', 'O'), ('and', 'O', 'O'), ('that', 'O', 'O'), ('was', 'O', 'O'), ('even', 'O', 'O'), ('too', 'O', 'O'), ('much', 'O', 'O'), ('for', 'O', 'O'), ('my', 'O', 'O'), ('boyfriend', 'O', 'O'), ('and', 'O', 'O'), ('i', 'O', 'O'), ('to', 'O', 'O'), ('share', 'O', 'O'), ('.', 'O', 'O'), ('</s>', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "samples = du.sample_wrong_predicts(y_pred, y_train, X_train, word_to_num)\n",
    "for sample in du.translate_samples(samples, num_to_word, tagnames):\n",
    "    print \"===============\"\n",
    "    print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = hidden.get_weights()[0].T\n",
    "L = embedding.get_weights()[0]\n",
    "b1 = hidden.get_weights()[1]\n",
    "b2 = output.get_weights()[1]\n",
    "U = output.get_weights()[0].T\n",
    "def print_scores(scores, words):\n",
    "    for i in reversed(range(len(scores))):\n",
    "        print \"[%d]: (%.03f) %s\" % (i, scores[i], words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "topscores = []\n",
    "topwords = []\n",
    "scores = W[:,100:150].dot(L.T)\n",
    "sort_idx = scores.argsort(axis=1)\n",
    "for i in xrange(scores.shape[0]):\n",
    "    topscores.append([])\n",
    "    topwords.append([])\n",
    "    for j in sort_idx[i][-10:]:\n",
    "        topscores[i].append(scores[i][j])\n",
    "        topwords[i].append(num_to_word[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 46\n",
      "[9]: (0.252) yuh\n",
      "[8]: (0.248) wolf\n",
      "[7]: (0.247) gall\n",
      "[6]: (0.246) folles\n",
      "[5]: (0.243) rooster\n",
      "[4]: (0.240) shoulder\n",
      "[3]: (0.238) brith\n",
      "[2]: (0.234) marrow\n",
      "[1]: (0.233) fuh\n",
      "[0]: (0.229) bleaching\n",
      "Neuron 54\n",
      "[9]: (0.456) carbohydrates\n",
      "[8]: (0.449) variables\n",
      "[7]: (0.428) integer\n",
      "[6]: (0.408) average\n",
      "[5]: (0.407) calories\n",
      "[4]: (0.403) tothis\n",
      "[3]: (0.396) statistically\n",
      "[2]: (0.393) computers\n",
      "[1]: (0.392) households\n",
      "[0]: (0.390) arithmetic\n",
      "Neuron 204\n",
      "[9]: (0.193) crawfish\n",
      "[8]: (0.142) kimchi\n",
      "[7]: (0.139) falafel\n",
      "[6]: (0.137) liters\n",
      "[5]: (0.136) gallon\n",
      "[4]: (0.135) medium\n",
      "[3]: (0.130) solanum\n",
      "[2]: (0.130) tempranillo\n",
      "[1]: (0.129) arabica\n",
      "[0]: (0.129) parf\n",
      "Neuron 122\n",
      "[9]: (0.471) </s>\n",
      "[8]: (0.286) haitians\n",
      "[7]: (0.270) residing\n",
      "[6]: (0.260) hard-of-hearing\n",
      "[5]: (0.243) apathetic\n",
      "[4]: (0.240) filipinos\n",
      "[3]: (0.239) mediterranean-type\n",
      "[2]: (0.239) orleanian\n",
      "[1]: (0.238) dominicans\n",
      "[0]: (0.238) cubans\n",
      "Neuron 102\n",
      "[9]: (0.381) quaalude\n",
      "[8]: (0.367) deez\n",
      "[7]: (0.330) bratwursts\n",
      "[6]: (0.328) ljr\n",
      "[5]: (0.308) bulgogi\n",
      "[4]: (0.304) crawfish\n",
      "[3]: (0.304) woogie\n",
      "[2]: (0.303) redenbacher\n",
      "[1]: (0.285) sauerkraut\n",
      "[0]: (0.277) goong\n"
     ]
    }
   ],
   "source": [
    "neurons = [46,54,204,122,102] # change this to your chosen neurons\n",
    "for i in neurons:\n",
    "    print \"Neuron %d\" % i\n",
    "    print_scores(topscores[i], topwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 250) (69740, 50) (300,) (5,) (5, 300)\n"
     ]
    }
   ],
   "source": [
    "print W.shape, L.shape, b1.shape, b2.shape, U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['equations', 'equation', 'linear', 'differential', 'variables', 'approximation', 'parameters', 'discrete', 'geometry', 'generalization', 'mathematical', 'algebraic', 'generalized', 'finite', 'approximations', 'equilibrium', 'probability', 'algorithms', 'constants', 'quantum', 'semantics']]\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "wv_trained = embedding.get_weights()[0]\n",
    "print wordvector_neighbors([word_to_num['equations']], wv_trained, num_to_word, n=20)\n",
    "print word_to_num['ordered']\n",
    "#print model.predict_classes(array([[75], [2226]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9831.0\n",
      "[('<s>', 1322.0), ('the', 601.0), (',', 436.0), ('and', 414.0), ('had', 271.0), ('a', 239.0), ('with', 222.0), ('UUUNKKK', 193.0), ('ordered', 188.0), ('of', 185.0), ('for', 134.0), ('got', 107.0), ('to', 89.0), ('was', 89.0), ('i', 82.0), ('on', 70.0), ('order', 66.0), ('is', 61.0), ('but', 52.0), ('in', 49.0), ('try', 49.0), ('get', 48.0), ('their', 48.0), ('that', 48.0), ('like', 45.0), ('rice', 41.0), ('we', 40.0), ('cheese', 39.0), ('love', 38.0), ('pizza', 38.0), ('tried', 37.0), ('side', 37.0), ('my', 37.0), ('-', 34.0), ('have', 33.0), ('salad', 33.0), ('d', 33.0), ('as', 31.0), ('or', 31.0), ('good', 28.0), ('about', 28.0), (':', 28.0), ('chicken', 28.0), ('out', 27.0), ('shrimp', 27.0), ('...', 27.0), ('it', 27.0), ('$', 24.0), ('enjoyed', 24.0), ('m', 24.0), (\"'s\", 24.0), ('tuna', 23.0), ('coconut', 23.0), ('salmon', 22.0), ('they', 22.0), ('from', 21.0), ('bacon', 21.0), ('your', 20.0), ('fries', 20.0), ('eggs', 20.0), ('were', 20.0), ('not', 19.0), (\"n't\", 19.0), ('soup', 19.0), ('mac', 18.0), ('really', 18.0), ('sausage', 18.0), ('roll', 18.0), ('burger', 18.0), ('some', 18.0), ('are', 18.0), ('also', 18.0), ('loved', 17.0), ('cream', 17.0), ('fresh', 17.0), ('came', 17.0), ('slice', 17.0), ('pieces', 16.0), ('beans', 16.0), ('an', 16.0), ('crab', 16.0), ('recommend', 15.0), ('be', 15.0), ('mustard', 15.0), ('piece', 15.0), ('than', 15.0), ('you', 15.0), ('up', 14.0), ('fish', 14.0), ('very', 14.0), ('beef', 13.0), ('dd', 13.0), ('just', 13.0), ('make', 13.0), (';', 13.0), ('been', 13.0), ('cup', 12.0), ('&', 12.0), ('went', 12.0), ('curry', 12.0), ('salsa', 12.0), ('sauce', 12.0), ('gyro', 12.0), ('all', 12.0), ('best', 12.0), ('comes', 11.0), ('meat', 11.0), ('shared', 11.0), ('sushi', 11.0), ('at', 11.0), ('pitcher', 11.0), ('food', 11.0), ('things', 10.0), ('amount', 10.0), ('w', 10.0), ('egg', 10.0), ('me', 10.0), ('``', 10.0), ('liked', 10.0), ('here', 10.0), ('ordering', 10.0), ('dog', 10.0), ('steak', 10.0), ('which', 10.0), ('calamari', 10.0), ('even', 10.0), ('both', 9.0), ('pineapple', 9.0), ('rolls', 9.0), ('off', 9.0), ('when', 9.0), ('slices', 9.0), ('al', 9.0), ('if', 9.0), ('avocado', 9.0), ('great', 9.0), ('thick', 9.0), ('menu', 9.0), (\"''\", 9.0), ('spicy', 9.0), ('then', 8.0), ('us', 8.0), ('mayo', 8.0), ('time', 8.0), ('so', 8.0), ('more', 8.0), ('who', 8.0), ('fan', 8.0), ('sandwich', 8.0), ('small', 8.0)]\n"
     ]
    }
   ],
   "source": [
    "count = zeros(len(word_to_num))\n",
    "for idx in xrange(len(X_train)):\n",
    "    if y_train[idx] == 4:\n",
    "        count[X_train[idx][0]] += 1\n",
    "    \n",
    "print sum(count)\n",
    "print [(num_to_word[i], count[i]) for i in argsort(count)[::-1][:150]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output neuron 1: B\n",
      "[19]: (0.898) yucatan\n",
      "[18]: (0.841) kavkaz\n",
      "[17]: (0.828) udd\n",
      "[16]: (0.823) banh\n",
      "[15]: (0.816) trahn\n",
      "[14]: (0.804) queso\n",
      "[13]: (0.794) nem\n",
      "[12]: (0.792) corazon\n",
      "[11]: (0.790) veel\n",
      "[10]: (0.785) evo\n",
      "[9]: (0.782) buckeye\n",
      "[8]: (0.775) loo\n",
      "[7]: (0.772) mauricio\n",
      "[6]: (0.766) louisiana\n",
      "[5]: (0.765) mih\n",
      "[4]: (0.757) az\n",
      "[3]: (0.753) boh\n",
      "[2]: (0.752) toppled\n",
      "[1]: (0.748) mee\n",
      "[0]: (0.734) shirted\n",
      "\n",
      "Output neuron 2: I\n",
      "[19]: (0.709) del\n",
      "[18]: (0.675) highmark\n",
      "[17]: (0.601) malgr\n",
      "[16]: (0.562) wwf\n",
      "[15]: (0.544) marrow\n",
      "[14]: (0.543) kickstarter\n",
      "[13]: (0.542) di\n",
      "[12]: (0.539) glacier\n",
      "[11]: (0.526) rbc\n",
      "[10]: (0.517) dore\n",
      "[9]: (0.505) pond\n",
      "[8]: (0.501) templeton\n",
      "[7]: (0.496) poppy\n",
      "[6]: (0.493) blotch\n",
      "[5]: (0.491) somme\n",
      "[4]: (0.489) diamond\n",
      "[3]: (0.488) director\n",
      "[2]: (0.487) lloyd\n",
      "[1]: (0.482) gall\n",
      "[0]: (0.481) hamon\n",
      "\n",
      "Output neuron 3: L\n",
      "[19]: (0.892) shoulder\n",
      "[18]: (0.884) quaalude\n",
      "[17]: (0.874) supermarionation\n",
      "[16]: (0.868) wolfpack\n",
      "[15]: (0.844) myoo\n",
      "[14]: (0.826) gem\n",
      "[13]: (0.819) bravas\n",
      "[12]: (0.806) rings\n",
      "[11]: (0.798) grosset\n",
      "[10]: (0.796) refresher\n",
      "[9]: (0.795) gladiator\n",
      "[8]: (0.777) fritos\n",
      "[7]: (0.776) brah\n",
      "[6]: (0.774) heck\n",
      "[5]: (0.771) stephenie\n",
      "[4]: (0.771) topps\n",
      "[3]: (0.755) razzie\n",
      "[2]: (0.748) chino\n",
      "[1]: (0.743) nemo\n",
      "[0]: (0.731) bueller\n",
      "\n",
      "Output neuron 4: U\n",
      "[19]: (0.991) crawfish\n",
      "[18]: (0.930) sauerkraut\n",
      "[17]: (0.929) kimchi\n",
      "[16]: (0.911) bratwursts\n",
      "[15]: (0.866) etouffee\n",
      "[14]: (0.862) mild\n",
      "[13]: (0.860) falafel\n",
      "[12]: (0.855) tempranillo\n",
      "[11]: (0.854) bulgogi\n",
      "[10]: (0.848) asian/pacific\n",
      "[9]: (0.835) rosti\n",
      "[8]: (0.834) arabica\n",
      "[7]: (0.830) machaca\n",
      "[6]: (0.829) manchego\n",
      "[5]: (0.826) coffee\n",
      "[4]: (0.821) homero\n",
      "[3]: (0.818) mykonos\n",
      "[2]: (0.814) ljr\n",
      "[1]: (0.810) learnings\n",
      "[0]: (0.801) zealands\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import softmax\n",
    "h = tanh(L.dot(W[:,100:150].T) + b1)\n",
    "y_hat = softmax(h.dot(U.T) + b2)\n",
    "sort_id = y_hat.argsort(axis=0)\n",
    "topscores = [[] for _ in range(5)]\n",
    "topwords = [[] for _ in range(5)]\n",
    "for i in xrange(5):\n",
    "    for j in sort_id.T[i][-20:]:\n",
    "        topscores[i].append(y_hat[j][i])\n",
    "        topwords[i].append(num_to_word[j])\n",
    "\n",
    "for i in range(1,5):\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(topscores[i], topwords[i])\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62490\n",
      "Output neuron 1: B\n",
      "[29]: (0.796) equations\n",
      "[28]: (0.781) variables\n",
      "[27]: (0.763) constants\n",
      "[26]: (0.756) resend\n",
      "[25]: (0.743) axioms\n",
      "[24]: (0.713) dimensionless\n",
      "[23]: (0.700) finite\n",
      "[22]: (0.677) parameter\n",
      "[21]: (0.674) quantum\n",
      "[20]: (0.671) discriminant\n",
      "[19]: (0.670) thermodynamic\n",
      "[18]: (0.646) equilibrium\n",
      "[17]: (0.646) equation\n",
      "[16]: (0.638) arithmetic\n",
      "[15]: (0.629) matrix\n",
      "[14]: (0.623) algebraic\n",
      "[13]: (0.615) mathematical\n",
      "[12]: (0.613) equivalence\n",
      "[11]: (0.612) infinite\n",
      "[10]: (0.608) cognitive\n",
      "[9]: (0.608) vectors\n",
      "[8]: (0.607) probability\n",
      "[7]: (0.602) discrete\n",
      "[6]: (0.602) algorithms\n",
      "[5]: (0.599) extenuating\n",
      "[4]: (0.596) differential\n",
      "[3]: (0.593) ;p\n",
      "[2]: (0.592) relativity\n",
      "[1]: (0.587) spatial\n",
      "[0]: (0.584) tothis\n",
      "\n",
      "Output neuron 2: I\n",
      "[29]: (0.703) hah\n",
      "[28]: (0.689) finnegans\n",
      "[27]: (0.643) petrel\n",
      "[26]: (0.641) muh\n",
      "[25]: (0.637) dri\n",
      "[24]: (0.632) cedars\n",
      "[23]: (0.616) shinzo\n",
      "[22]: (0.613) seki\n",
      "[21]: (0.607) sheh\n",
      "[20]: (0.607) typhoon\n",
      "[19]: (0.599) banh\n",
      "[18]: (0.585) sago\n",
      "[17]: (0.583) cyclone\n",
      "[16]: (0.580) mele\n",
      "[15]: (0.580) vieille\n",
      "[14]: (0.567) thae\n",
      "[13]: (0.565) orogeny\n",
      "[12]: (0.564) edo\n",
      "[11]: (0.562) oberweis\n",
      "[10]: (0.560) cheeked\n",
      "[9]: (0.557) hawker\n",
      "[8]: (0.557) poco\n",
      "[7]: (0.555) yew\n",
      "[6]: (0.554) tsutomu\n",
      "[5]: (0.549) jaipal\n",
      "[4]: (0.547) landslide\n",
      "[3]: (0.542) ipsa\n",
      "[2]: (0.539) fausto\n",
      "[1]: (0.538) cheon\n",
      "[0]: (0.538) hipped\n",
      "\n",
      "Output neuron 3: L\n",
      "[29]: (0.917) veel\n",
      "[28]: (0.841) kavkaz\n",
      "[27]: (0.819) zeez\n",
      "[26]: (0.808) ---------------------\n",
      "[25]: (0.804) enh\n",
      "[24]: (0.772) dunant\n",
      "[23]: (0.767) canary\n",
      "[22]: (0.758) macht\n",
      "[21]: (0.748) yoh\n",
      "[20]: (0.741) mercat\n",
      "[19]: (0.740) bertolli\n",
      "[18]: (0.739) sucht\n",
      "[17]: (0.736) carnation\n",
      "[16]: (0.735) ----------------------\n",
      "[15]: (0.734) pablo\n",
      "[14]: (0.729) tusker\n",
      "[13]: (0.729) trahn\n",
      "[12]: (0.728) malecon\n",
      "[11]: (0.727) no-man\n",
      "[10]: (0.725) flank\n",
      "[9]: (0.719) luh\n",
      "[8]: (0.717) hakone\n",
      "[7]: (0.714) suizo\n",
      "[6]: (0.708) tverskaya\n",
      "[5]: (0.707) yuh\n",
      "[4]: (0.705) huevos\n",
      "[3]: (0.704) magnate\n",
      "[2]: (0.703) cay\n",
      "[1]: (0.703) sebastin\n",
      "[0]: (0.702) klaas\n",
      "\n",
      "Output neuron 4: U\n",
      "[29]: (0.196) vehicular\n",
      "[28]: (0.193) larceny\n",
      "[27]: (0.192) teb\n",
      "[26]: (0.189) crimes\n",
      "[25]: (0.186) third-person\n",
      "[24]: (0.181) theft\n",
      "[23]: (0.179) atrocities\n",
      "[22]: (0.176) suspicion\n",
      "[21]: (0.172) complicity\n",
      "[20]: (0.171) penetration\n",
      "[19]: (0.171) felony\n",
      "[18]: (0.170) deterring\n",
      "[17]: (0.169) impunity\n",
      "[16]: (0.169) criminals\n",
      "[15]: (0.168) evade\n",
      "[14]: (0.166) sophisticated\n",
      "[13]: (0.164) criminal\n",
      "[12]: (0.163) carnal\n",
      "[11]: (0.163) unforced\n",
      "[10]: (0.161) manslaughter\n",
      "[9]: (0.160) hand-to-hand\n",
      "[8]: (0.160) first-hand\n",
      "[7]: (0.159) apprehension\n",
      "[6]: (0.159) persecution\n",
      "[5]: (0.158) dispersion\n",
      "[4]: (0.157) superiority\n",
      "[3]: (0.157) systematic\n",
      "[2]: (0.156) fraulein\n",
      "[1]: (0.155) concentration\n",
      "[0]: (0.154) gainful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import softmax\n",
    "h = tanh(L.dot(W[:, 50:100].T) + b1)\n",
    "y_hat = softmax(h.dot(U.T) + b2)\n",
    "sort_id = y_hat.argsort(axis=0)\n",
    "topscores = [[] for _ in range(5)]\n",
    "topwords = [[] for _ in range(5)]\n",
    "for i in xrange(5):\n",
    "    for j in sort_id.T[i][-30:]:\n",
    "        topscores[i].append(y_hat[j][i])\n",
    "        topwords[i].append(num_to_word[j])\n",
    "print word_to_num['equations']\n",
    "\n",
    "for i in range(1,5):\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(topscores[i], topwords[i])\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69384\n",
      "69740\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(xrange(len(sort_id))):\n",
    "    if sort_id.T[1][i] == word_to_num['ordered']:\n",
    "        print i\n",
    "        break\n",
    "    \n",
    "print len(sort_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_label_for_X(X, Y):\n",
    "    X_label = zeros((len(X), windowSize/2 * 5))\n",
    "    for i in xrange(len(X)):\n",
    "        label = array([])\n",
    "        for j in xrange(windowSize/2):\n",
    "            if X[i][j] == word_to_num['<s>']:\n",
    "                label = append(label, zeros(5)) \n",
    "            else:\n",
    "                label = append(label, Y[i - windowSize/2 + j])\n",
    "        X_label[i] = label\n",
    "    return X_label\n",
    "X_train_label = get_label_for_X(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  1.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.])]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print Y_train[:15]\n",
    "print X_train_label[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 13948000 bytes of device memory (unspecified launch failure).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-a2844cc4991b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmiddleDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/Keras-0.0.1-py2.7.egg/keras/layers/embeddings.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, init, weights, W_regularizer, W_constraint)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW_constraint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/Keras-0.0.1-py2.7.egg/keras/initializations.pyc\u001b[0m in \u001b[0;36muniform\u001b[0;34m(shape, scale)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msharedX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/Keras-0.0.1-py2.7.egg/keras/utils/theano_utils.pyc\u001b[0m in \u001b[0;36msharedX\u001b[0;34m(X, dtype, name)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msharedX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshared_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/guoxing/code/Theano/theano/compile/sharedvalue.pyc\u001b[0m in \u001b[0;36mshared\u001b[0;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[0;32m--> 211\u001b[0;31m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/guoxing/code/Theano/theano/sandbox/cuda/var.pyc\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[0;34m(value, name, strict, allow_downcast, borrow, broadcastable)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# function requires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mdeviceval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ('Error allocating 13948000 bytes of device memory (unspecified launch failure).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "windowSize = 5\n",
    "middleDim = 300\n",
    "ntrain = len(X_train)\n",
    "\n",
    "\n",
    "left = Sequential()\n",
    "left.add(Embedding(wv.shape[0], wv.shape[1], weights=[wv]))\n",
    "left.add(Flatten())\n",
    "left.add(Dense(wv.shape[1] * windowSize, middleDim, W_regularizer = l2(.001)))\n",
    "\n",
    "right = Sequential()\n",
    "right.add(Dense(windowSize/2 * 5, middleDim, W_regularizer = l2(0.001)))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Merge([left, right], mode='sum'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(middleDim, 5, W_regularizer = l2(.001)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "%time model.fit([X_train[:ntrain], X_train_label[:ntrain]], Y_train[:ntrain], nb_epoch=10, batch_size=128)\n",
    "\n",
    "\n",
    "'''\n",
    "left = Sequential()\n",
    "left.add(Embedding(wv.shape[0], wv.shape[1], weights=[wv]))\n",
    "left.add(Flatten())\n",
    "left.add(Dense(wv.shape[1] * windowSize, middleDim, W_regularizer = l2(.001)))\n",
    "left.add(Activation('tanh'))\n",
    "\n",
    "right = Sequential()\n",
    "right.add(Dense(windowSize/2 * 5, 20, W_regularizer = l2(0.001)))\n",
    "right.add(Activation('tanh'))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Dense(middleDim + 20, 5, W_regularizer = l2(.001)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "%time model.fit([X_train[:ntrain], X_train_label[:ntrain]], Y_train[:ntrain], nb_epoch=10, batch_size=128)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequential_predict(X):\n",
    "    y_pred = []\n",
    "    prev = zeros((len(X), 5))\n",
    "    for i in xrange(len(X)):\n",
    "        label = array([])\n",
    "        for j in xrange(windowSize/2):\n",
    "            if X[i][j] == '<s>':\n",
    "                label = append(label, zeros(5))\n",
    "            else:\n",
    "                label = append(label, prev[i - windowSize/2 + j])\n",
    "        pred = model.predict_classes([[X[i]], [label]], batch_size=1, verbose=0)\n",
    "        y_pred.append(pred[0])\n",
    "        prev[i][pred[0]] = 1.0\n",
    "    return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.96      0.99      0.97    340758\n",
      "          B       0.83      0.64      0.72     12927\n",
      "          I       0.79      0.50      0.61      4883\n",
      "          L       0.81      0.62      0.70     12927\n",
      "          U       0.70      0.54      0.61      9831\n",
      "\n",
      "avg / total       0.94      0.94      0.94    381326\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  78.80%\n",
      "Mean recall:     59.32%\n",
      "Mean F1:         67.63%\n"
     ]
    }
   ],
   "source": [
    "y_pred = sequential_predict(X_train)\n",
    "nerbase.full_report(y_train, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_train, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9268    0.9680    0.9470     41047\n",
      "          B     0.6709    0.5406    0.5987      2157\n",
      "          I     0.6147    0.3922    0.4788       663\n",
      "          L     0.6628    0.5341    0.5915      2157\n",
      "          U     0.6469    0.4231    0.5116      2160\n",
      "\n",
      "avg / total     0.8867    0.8971    0.8895     48184\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  65.60%\n",
      "Mean recall:     48.93%\n",
      "Mean F1:         55.90%\n"
     ]
    }
   ],
   "source": [
    "y_pred = sequential_predict(X_dev)\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9267    0.9680    0.9469     41047\n",
      "          B     0.6807    0.5387    0.6014      2157\n",
      "          I     0.6080    0.3650    0.4562       663\n",
      "          L     0.6614    0.5234    0.5844      2157\n",
      "          U     0.6311    0.4380    0.5171      2160\n",
      "\n",
      "avg / total     0.8862    0.8968    0.8892     48184\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  65.31%\n",
      "Mean recall:     48.75%\n",
      "Mean F1:         55.73%\n"
     ]
    }
   ],
   "source": [
    "y_pred = sequential_predict(X_dev)\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_cap = array([array([x[0].isupper() for x in X]) for X in words_train]).astype(int)\n",
    "X_dev_cap = array([array([x[0].isupper() for x in X]) for X in words_dev]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0]\n",
      "['<s>' '<s>' 'Excellent' 'Chinese' 'food']\n",
      "[0 0 0 1 1]\n",
      "['<s>' '<s>' 'the' 'Lemon' 'Pepper']\n",
      "[False, False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "print X_train_cap[0]\n",
    "print words_train[0]\n",
    "print X_dev_cap[0]\n",
    "print words_dev[0]\n",
    "print [x[0].isupper() for x in words_dev[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "385851/385851 [==============================] - 57s - loss: 0.2939    \n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "385851/385851 [==============================] - 59s - loss: 0.2518    \n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "385851/385851 [==============================] - 58s - loss: 0.2376    \n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2289    \n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "385851/385851 [==============================] - 61s - loss: 0.2226    \n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2175    \n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "385851/385851 [==============================] - 62s - loss: 0.2137    \n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "385851/385851 [==============================] - 74s - loss: 0.2102    \n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "385851/385851 [==============================] - 66s - loss: 0.2073    \n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "385851/385851 [==============================] - 60s - loss: 0.2047    \n",
      "CPU times: user 9min 56s, sys: 45.2 s, total: 10min 41s\n",
      "Wall time: 10min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': [],\n",
       " 'epoch': 9,\n",
       " 'epoch_size': [],\n",
       " 'loss': [0.29394582775457706,\n",
       "  0.54570444892527992,\n",
       "  0.78330673061229739,\n",
       "  1.0122426433710299,\n",
       "  1.2348388123407061,\n",
       "  1.4523808757283934,\n",
       "  1.666053215171077,\n",
       "  1.8762421214010574,\n",
       "  2.0835072846690208,\n",
       "  2.2881705725199488],\n",
       " 'val_accuracy': [],\n",
       " 'val_loss': []}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "windowSize = 5\n",
    "middleDim = 300\n",
    "ntrain = len(X_train)\n",
    "\n",
    "left = Sequential()\n",
    "left.add(Embedding(wv.shape[0], wv.shape[1], weights=[wv]))\n",
    "left.add(Flatten())\n",
    "left.add(Dense(wv.shape[1] * windowSize, middleDim, W_regularizer = l2(.001)))\n",
    "\n",
    "right = Sequential()\n",
    "right.add(Dense(windowSize, middleDim, W_regularizer = l2(0.001)))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Merge([left, right], mode='sum'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(middleDim, 5, W_regularizer = l2(.001)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')\n",
    "%time model.fit([X_train[:ntrain], X_train_cap[:ntrain]], Y_train[:ntrain], nb_epoch=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48184/48184 [==============================] - 0s     \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O     0.9267    0.9674    0.9466     41047\n",
      "          B     0.6415    0.5990    0.6195      2157\n",
      "          I     0.5873    0.4872    0.5326       663\n",
      "          L     0.6739    0.5012    0.5748      2157\n",
      "          U     0.6744    0.3644    0.4731      2160\n",
      "\n",
      "avg / total     0.8867    0.8964    0.8884     48184\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  65.62%\n",
      "Mean recall:     48.80%\n",
      "Mean F1:         55.36%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes([X_dev, X_dev_cap])\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WindowMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nepoch = 5\n",
    "N = nepoch * len(y_train)\n",
    "k = 5 # minibatch size\n",
    "print wv.shape\n",
    "\n",
    "\n",
    "random.seed(10) # do not change this!\n",
    "#### YOUR CODE HERE ####\n",
    "def epochSchedule(n):\n",
    "    for i in xrange(n):\n",
    "        yield i % len(y_train)\n",
    "        \n",
    "def randomSchedule(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "        yield random.randint(0, len(y_train))\n",
    "        \n",
    "def randomMinibatchSchedule(n, k):\n",
    "    i = 0\n",
    "    while i < n / k:\n",
    "        i += 1\n",
    "        yield [random.randint(0, len(y_train)) for _ in xrange(k)]\n",
    "        \n",
    "def annealingAlpha(n, alpha, tau):\n",
    "    for i in xrange(n):\n",
    "        yield alpha * tau / max(i, tau)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952300\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 3.06684\n",
      "  [9523]: mean loss 0.165779\n",
      "SGD complete: 9523 examples in 307.86 seconds.\n",
      "CPU times: user 5min 5s, sys: 3.15 s, total: 5min 9s\n",
      "Wall time: 5min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 3.0668426776904845), (9523, 0.16577878826707726)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nerwindow import WindowMLP\n",
    "clf = WindowMLP(wv, windowsize=5, dims=[None, 200, 5],\n",
    "                reg=0.001, alpha=0.01)\n",
    "n = 10 * len(X_train)\n",
    "print n\n",
    "k = 100\n",
    "%time clf.train_sgd(X=X_train, y=y_train, idxiter=randomMinibatchSchedule(n, k), alphaiter=annealingAlpha(n / k, 0.01, n / k / 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.98      0.97     85505\n",
      "          B       0.80      0.70      0.74      3043\n",
      "          I       0.71      0.48      0.58       910\n",
      "          L       0.73      0.74      0.74      3043\n",
      "          U       0.76      0.57      0.65      2729\n",
      "\n",
      "avg / total       0.95      0.95      0.95     95230\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  75.88%\n",
      "Mean recall:     65.72%\n",
      "Mean F1:         70.11%\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "nerbase.full_report(y_train, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_train, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.93      0.97      0.95      8559\n",
      "          B       0.43      0.35      0.39       255\n",
      "          I       0.38      0.15      0.21       135\n",
      "          L       0.40      0.42      0.41       255\n",
      "          U       0.34      0.09      0.14       328\n",
      "\n",
      "avg / total       0.87      0.90      0.88      9532\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  38.38%\n",
      "Mean recall:     25.39%\n",
      "Mean F1:         28.66%\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_dev)\n",
    "nerbase.full_report(y_dev, y_pred, tagnames)\n",
    "nerbase.eval_performance(y_dev, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# baseline features:\n",
    "\n",
    "# context\n",
    "# capitalize word\n",
    "# previous tokens...\n",
    "##### FEATURES TO ADD #####\n",
    "def isCapital(x):\n",
    "    if len(x) > 0:\n",
    "        return x[0].isupper()\n",
    "    return false\n",
    "\n",
    "def extractFeatures(X, words, windowsize, word_to_num):\n",
    "    fv1 = contextFeature(X, windowsize, word_to_num)\n",
    "    fv2 = capitalFeature(words)\n",
    "    fv = column_stack((fv1,fv2))\n",
    "    return fv2\n",
    "\n",
    "def contextFeature(X, windowsize, word_to_num):\n",
    "    m = len(word_to_num)\n",
    "    onehot_x = identity(m)\n",
    "\n",
    "    wdim = m * windowsize\n",
    "    N = X.shape[0]\n",
    "    fvec = onehot_x[X].reshape(N, wdim)\n",
    "    return fvec\n",
    "\n",
    "def capitalFeature(words):\n",
    "    fvec = array([isCapital(ww) for ww in words]).astype(int)\n",
    "    return fvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num)\n",
    "def trainSubset(size, X_train, y_train, words_train):\n",
    "    X_train_s = X_train[:size]\n",
    "    y_train_s = y_train[:size]\n",
    "    words_train_s = words_train[:size]\n",
    "    return X_train_s, y_train_s, words_train_s\n",
    "\n",
    "\n",
    "# TRAIN_SIZE = 500\n",
    "# X_s, y_s, w_s = trainSubset(TRAIN_SIZE, X_train, y_train, words_train)\n",
    "X_s = X_train\n",
    "y_s = y_train\n",
    "w_s = words_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv = extractFeatures(X_s, w_s, wsize, word_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### just trying###\n",
    "X = X_train\n",
    "m = len(word_to_num)\n",
    "onehot_x = identity(m)\n",
    "\n",
    "wdim = m * wsize\n",
    "N = X.shape[0]\n",
    "fvec = onehot_x[X].reshape(N, wdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95230,)\n"
     ]
    }
   ],
   "source": [
    "print fv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using sklearn logistic regression package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      1.00      0.99       430\n",
      "          B       1.00      0.79      0.88        19\n",
      "          I       1.00      0.85      0.92        13\n",
      "          L       1.00      0.84      0.91        19\n",
      "          U       1.00      0.79      0.88        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97       500\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(fv, y_s)\n",
    "y_pred = lr.predict(fv)\n",
    "print nerbase.full_report(y_s, y_pred, tagnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8486\n",
      "9082\n"
     ]
    }
   ],
   "source": [
    "print len(X_dev)\n",
    "print len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29989)\n",
      "(8486, 29989)\n"
     ]
    }
   ],
   "source": [
    "print fv.shape\n",
    "print fvdev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing using dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.88      1.00      0.93      7392\n",
      "          B       0.25      0.00      0.00       403\n",
      "          I       0.00      0.00      0.00       196\n",
      "          L       0.00      0.00      0.00       403\n",
      "          U       0.09      0.05      0.07        92\n",
      "\n",
      "avg / total       0.78      0.87      0.81      8486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fvdev = extractFeatures(X_dev, words_dev, windowsize, word_to_num)\n",
    "yp = lr.predict(fvdev)\n",
    "\n",
    "nerbase.full_report(y_dev, yp, tagnames) # full report, helpful diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
